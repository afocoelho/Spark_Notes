{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark =SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Sample_EDA\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.parquet('data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark\n",
    "Spark is a computing system used on clusters which is based on memory(RAM).\n",
    "Spark fundamental data structure are RDDs. In order to construct those, the data is load from distributed file system as HDFS or EMRFS to memory partition of typical 128MB.  \n",
    "**How it works**:  \n",
    "\n",
    "   1. Loads the data to RDDs\n",
    "   2. User Submits APP to file system client and Resource manager(RM):\n",
    "       - Resoure manager defines a container to run a application master (AM)  \n",
    "         \n",
    "       - AM and RM comunicate and allocate conainers and resources to the Driver program and the Executor program. ( Driver program containts the spark context which will send information to both worker nodes and Cluster Manager as it possible to see in the image bellow)  \n",
    "         \n",
    "       - Spark and Appliaction Manager and RM comunicate to update the files and hive metadata.  \n",
    "         \n",
    "       - AM tells RM that the job is finished and RM deallocates the resources.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/afocoelho/Spark_Notes/main/media/SparkContext.PNG\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Where is our data?**  \n",
    "   In our case the data is in a Amazon S3 Object Storage Service which is managed by Elastic Map Reduce File System (EMRFS) which is the implementation of hadoop on the Amazon S3 disk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Shell\n",
    "to access write pyspark  \n",
    "Commands:  \n",
    "  - `master`: Specifies a cluster to connect.\n",
    "    - Possible values:\n",
    "      - yarn\n",
    "      - spark://masternode:port ( Spark standalone )\n",
    "      - mesos://masternode:port ( Mesos standalone ) Mesos is a cluster computing system\n",
    "      - local  [ \\* ] | runs locally (* defines the number of threads)\n",
    "  - `jars`: additional JAR files.\n",
    "  - `py-files`: Additional Python Files\n",
    "  - `name`: application name.`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark RDD Concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition: Resilient Distributed dataset\n",
    "RDDs are the first level of abstraction of spark. They are written in SCALA and they run on a JVM.\n",
    "RDD as the name say is :\n",
    "   - Resilient - Each time a processing occurs the map of this processing is kept it and if a partition dies the system automaticaly reproduces the data present in that partition.   \n",
    "   - Distributed - Data Stays and it is processed on multiple nodes in a cluster.  \n",
    "   - Dataset - Data is initially in a primitive format as like tuples created either from a source or programmatically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RDD Characteristics\n",
    "`In-Memory`:Data in a RDD is stored in memory as long as it is needed.  \n",
    "  \n",
    "`Immutable`:It is not possible to change the data inside an rdd. Every time we which to make a transformation we are actually creating a new RDD.  \n",
    "  \n",
    "`Lazy evaluated`:The data inside an RDD is not able or transformed until an action that starts the execution is executed.   \n",
    "  \n",
    "`Cacheable`: All the data can either stay on memory or in a disk. Although disk are not preferible once they have a much lower access speed.  \n",
    "  \n",
    "`Parallel`: Data is process in parallel.  \n",
    "  \n",
    "`Type`: RDD records have types. ( RDD\\[int, str\\] , RDD\\[long\\])  \n",
    "  \n",
    "`Partitioned`: Records are split into logical partitions and distributed across nodes in a cluster.\n",
    "  \n",
    "`Local-Sticknesss`: RDD can define placement preferences to compute partitions ( as close to the records as possible)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exectution: Resilient Distributed dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PIPELINE  \n",
    "Transformations RDD object are defined through an API.  \n",
    "Spark reads the transformations and creates a Directed Acyclic Graph Scheduler.  \n",
    "Then the task is passed and manage to a worker/executor by a task scheduler.  \n",
    "Task scheduler is responsible by guaranteeing that that specific task is well processed which includes retrying if something goes wrong."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Execution components\n",
    "**JOB**  \n",
    "The complete process of the data in spark can be seen as a Job.  \n",
    "**Stages**  \n",
    "Jobs have, then, inside them different stages.    \n",
    "The necessity of existing stages is defined according with the possibility of processing the data in each partition or the need of joining everything togehter. In order words. If there are two narrow transformations they can be performed in the same stage, although if there is a narrow transformation and then a wide transformation two stages will be created.  \n",
    "**Task**   \n",
    "Task is the lowest level of abstraction that is recommend to understand spark. Basically a task is an action that is applied in a partition of an RDD. Typically the same task is applied to all the partitions of an RDD but run in paralell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Partitions\n",
    "Partitions are blocks of information from which the data will be processed.  \n",
    "By definition when reading from a distributed file system a partition will be created for every existing block in the system that it is needed for the creation of that specific RDD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tranformations\n",
    "Transformations are lazy operations, which basically means that each time the a transformation is written using the API it will be written on the DAGscheduler and then when an tigger command is executed the graph will be executed.  \n",
    "There are 2 types of operations:\n",
    "   - **Narrow transformations:** Narrow transformations can be applied to each partition idenpendently creating the new partitions idependently in th enew RDD. Spark groups all the narrow transformations into a stage which is also called pipelining.\n",
    "       - Map\n",
    "       - Filter\n",
    "       - Union\n",
    "       - Join with inputs co-partioned\n",
    "   - **Wide transformations:** Wide transformations require that the data is passed through the network \n",
    "       - GroupByKey\n",
    "       - Join with inputs not co-partioned  \n",
    "         \n",
    "**NOTE:** RDDs by nature are just a value per element, but spark is specially prepared to understand some times of this elements.  \n",
    "That said RDDs which have defined by a tuple with a key and a value are interpreted by spark as key-value pairRDDS.On the other hand if the tuple are just two numerical values theyare interpreted by spark as DoubleRDDs.  \n",
    "\n",
    "??That said there are different functions for different rdds. Thats why it exists ByKey extension to the name of some funcitons.??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RDD Narrow Transformations\n",
    "**MAP Functions**\n",
    "Map functions have into account 2 things:\n",
    "   - Map function: which can be 3 types:\n",
    "       - Normal Map Function  \n",
    "       - Exploding Map Function  \n",
    "       - Aggregation Map Function   \n",
    "   - Partitions  \n",
    "   \n",
    "**Filter Functions**  \n",
    "  \n",
    "Filter function selects elements that satisfy a given condition\n",
    "  \n",
    "**Union**  \n",
    "Union is equivalent to append. \n",
    "  \n",
    "**Join**  \n",
    "Joins the values by keys.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(20190630, '2019/06/30'),\n",
       " (20190430, '2019/04/30'),\n",
       " (20190531, '2019/05/31'),\n",
       " (20190331, '2019/03/31')]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## MAP functions\n",
    "data_treated = df.rdd.map(lambda x:(x['ID_DATE'],str(x['ID_DATE'])[0:4]+'/'+str(x['ID_DATE'])[4:6]+'/'+str(x['ID_DATE'])[6:8]))\\\n",
    "        .distinct()\n",
    "data_treated.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Filter Functions\n",
    "data_filter = data_treated.filter(lambda x: '30' in x[1]).map(lambda x: (x[0],x[1]+'_filtered'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(20190630, '2019/06/30_filtered'), (20190430, '2019/04/30_filtered')]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_filter.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(20190630, '2019/06/30'),\n",
       " (20190430, '2019/04/30'),\n",
       " (20190531, '2019/05/31'),\n",
       " (20190331, '2019/03/31'),\n",
       " (20190630, '2019/06/30'),\n",
       " (20190430, '2019/04/30')]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Union\n",
    "data_treated.union(data_filter).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(20190430, ('2019/04/30', '2019/04/30_filtered')),\n",
       " (20190630, ('2019/06/30', '2019/06/30_filtered'))]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Join\n",
    "data_treated.join(data_filter).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RDD Wide transformations\n",
    "**GroupBy**  \n",
    "Groups all the elements.\n",
    "This function will need the user to then define an aggregation function. Like this:\n",
    "rdd.groupby().agg(aggregation function)\n",
    "Group by loads the data to all the network and then process it.\n",
    "On the other hand reduce by groups by inside each partition and then groups by those results throught the network into a final result.  \n",
    "**ReduceBY**  \n",
    "ReduceBy function\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  - .reduceByKey(lambda v1,v2 : function): where v1 is the log/value $i$ and v2 is the log/value  $i+1$,  $i \\space \\in \\space {0,len(RDD)}$ \n",
    "  - .groupByKey().`function` : in this case we used the `mapValues` function which can be defined:\n",
    "  - .mapValues(lambda a:function): here you get an iterator `a` which can be used to generator lists and apply other methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['20190430 wowow ', '20190531 wowow ', '20190331 wowow ', '20190630 wowow ']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select('ID_DATE').rdd.map(lambda x: str(x[0])+ ' wowow ').distinct().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pair RDD functions  \n",
    "Spark as specific RDD functions for key-value pairs RDDs  \n",
    "Common ways to create a pairRDD:  \n",
    "  - `rdd.map`  \n",
    "    \n",
    "  - `rdd.keyBy`   \n",
    "    \n",
    "  - `rdd.flatMap`  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions:\n",
    "  - `rdd.countByKey()` : returns a map with the count of occurrences of each key\n",
    "    \n",
    "  - `rdd.groupByKey()` : Groups all the values for each key in an RDD \n",
    "    \n",
    "  - `rdd.sortByKey()` : Sorts in ascending or descending order\n",
    "    \n",
    "  - `rdd.join()` : returns a RDD containing all pairs with matching keys from two RDDs\n",
    "    \n",
    "  - `rdd.lefOuterJoin()` / `rdd.rightOuterJoin()` / `rdd.fullOuterJoin()`\n",
    "    \n",
    "  - `rdd.mapValues()` : executes a function on values only, keeping the key the same (also applicable for flatMapValues)\n",
    "    \n",
    "  - `rdd.lookup(key)` : returns the value(s) for a key as a list \n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Spark SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark SQL is a module of spark designed to process structured data.\n",
    "The data is represent by two different types of abstractions:\n",
    "  - DataFrame\n",
    "  - Dataset  \n",
    "  \n",
    "Both use the same execution engine to compute the result.   \n",
    "Spark Catalyst is a library built as a rule-based system. And each rule focusses on the specific optimization.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Data Frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is?\n",
    "Spark data frames are a level of abstraction which can be interpreted as a pandas dataframe in python. The difference is that when an action is executed all the depending transformations written before will be applied in an optimized way. This is done by passing all the code into a optimzed graph which is then executed using map-reduce functions.????\n",
    "\n",
    "\n",
    "\n",
    "Spark catalyst optimizer executes query plans, and it executes the queries on RDDs.\n",
    "\n",
    "it is an extension of RDDs with better levels of abstraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Features of DataFrames:  \n",
    "  - Hability to process a great amount of data as KiloBytes PentaBytes...\n",
    "  - Hability to connect to different sources of information simultaneously like SQL databases Non SQL databases csv files...etc \n",
    "  - State of the Art code optimization and code generation through Spark Catalyst optimizer (tree tranfosrmation framework\n",
    "  - Can be easily integrated with Big-Data tools and frameworks with Spark.\n",
    "  - Provides good API for python, R, Scala and Java programming."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1 Data Frames Schemas  \n",
    "A dataframe always have is defined by a schema.\n",
    "A Schema is basicaly a representation of dataframe inside spark.\n",
    "Schema needs to be fine when the dataframe is created, ahtough they can be defined automatically by infering column types\n",
    "  \n",
    "  \n",
    "  \n",
    "Schemas are composed of pyspark.sql.types.StructType, that contain a list of pyspark.sql.types.StructFields, which is a class that can represent multiple data types (e.g., String, Integer). Each StructFields has:\n",
    "\n",
    "  - name – string, name of the field.\n",
    "  - dataType – DataType of the field. Must be instantiated into a specific DataType (e.g. StringType)\n",
    "  - nullable – boolean, whether the field can be null (None) or not.\n",
    "  - metadata – a dict from string to simple type that can be toInternald to JSON automatically\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import types as T # convetion is to call types T\n",
    "data = [(1,'Valter',100),(2,'Manel',150),(3,'Asdrubal',200)]\n",
    "fields = [\n",
    "    T.StructField('id',T.IntegerType(),False),# this boolean defines if this field accepts nullable\n",
    "    T.StructField('Name', T.StringType()),\n",
    "    T.StructField('value',T.IntegerType())\n",
    "]\n",
    "schema = T.StructType(fields)\n",
    "schema\n",
    "df = spark.createDataFrame(data,schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(id,IntegerType,false),StructField(Name,StringType,true),StructField(value,IntegerType,true)))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[StructField(id,IntegerType,false),\n",
       " StructField(Name,StringType,true),\n",
       " StructField(value,IntegerType,true)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.schema.fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id IntegerType False\n"
     ]
    }
   ],
   "source": [
    "print(df.schema.fields[0].name,df.schema.fields[0].dataType ,df.schema.fields[0].nullable)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id', 'Name', 'value']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schema.fieldNames()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(data,schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rows\n",
    "`pyspark.sql.Row`: represents a row of data in a DataFrame where fields can be acessed as attributes\n",
    "Row function can be used to create a Row object using as argumnts the name of The column and $=$ sign and the data in the format which is supposed to be introduced in that row. This fields must be sorted.\n",
    "Fields inside the row can be acessed either in a ordinal way, like an array, or by name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id=1, Name='Valter', value=100),\n",
       " Row(id=1, Name='Manel', value=100),\n",
       " Row(id=1, Name='Asdrubal', value=100)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Columns\n",
    "Most of dataframe transformations require a column specification to do so the API as multiple ways of acessing the columns.  \n",
    "It is possible to access columns in multiple ways like:\n",
    "  - String\n",
    "  - DataFrame\n",
    "  - `pyspark.sql.functions.column`\n",
    "  - by index\n",
    "  - by named column?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  1|\n",
      "|  2|\n",
      "|  3|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## parte prática algo simples\n",
    "df.select('id').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expressions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark is compatible with most SQL-like expressions and column operations. These operations include:\n",
    "  - Arithmetic operators:+, -, %, /, *\n",
    "  \n",
    "\n",
    "  - Logical operators: >=, >, ==, <, <=, ==, !=, &&, ||, and, or, not\n",
    "  \n",
    "\n",
    "  - String functions: like, contains, substr\n",
    "\n",
    "  \n",
    "  - Data testing functions: isNull, isNotNull, NaN (not a number)\n",
    "\n",
    "  \n",
    "  - Sorting functions: asc, desc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()## returns the number of elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id=1, Name='Valter', value=100), Row(id=2, Name='Manel', value=150)]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n=2\n",
    "df.take(n)# take n elements from the dataframe. this selection is made from the first element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id=1, Name='Valter', value=100),\n",
       " Row(id=2, Name='Manel', value=150),\n",
       " Row(id=3, Name='Asdrubal', value=200)]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.collect() # collect sends all the preivous defined transformations to the optimizer which then executes and returns the result final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+-----+\n",
      "| id|    Name|value|\n",
      "+---+--------+-----+\n",
      "|  1|  Valter|  100|\n",
      "|  2|   Manel|  150|\n",
      "|  3|Asdrubal|  200|\n",
      "+---+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show() # executes all the actions as collect and shows the final result in a pretty table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'DataFrameWriter' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-50-d8974ac3ef08>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# saves data into a file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: 'DataFrameWriter' object is not callable"
     ]
    }
   ],
   "source": [
    "# saves data into a file\n",
    "##df.write...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  1|\n",
      "|  2|\n",
      "|  3|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('id').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id=1, Name='Valter', value=100)]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.where(df.id==1).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: int, Name: string, value: int]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.limit(n)# creates a dataframe with n rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+-----+\n",
      "| id|    Name|value|\n",
      "+---+--------+-----+\n",
      "|  3|Asdrubal|  200|\n",
      "|  2|   Manel|  150|\n",
      "|  1|  Valter|  100|\n",
      "+---+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.orderBy(df.Name).show()# orders the data frame by a column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`df.join(other data frame,on,how)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+-----+--------+-----+\n",
      "| id|    Name|value|    Name|value|\n",
      "+---+--------+-----+--------+-----+\n",
      "|  1|  Valter|  100|  Valter|  100|\n",
      "|  3|Asdrubal|  200|Asdrubal|  200|\n",
      "|  2|   Manel|  150|   Manel|  150|\n",
      "+---+--------+-----+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.join(df,'id','inner').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+-----+---+--------+-----+\n",
      "| id|    Name|value| id|    Name|value|\n",
      "+---+--------+-----+---+--------+-----+\n",
      "|  1|  Valter|  100|  1|  Valter|  100|\n",
      "|  1|  Valter|  100|  2|   Manel|  150|\n",
      "|  1|  Valter|  100|  3|Asdrubal|  200|\n",
      "|  2|   Manel|  150|  1|  Valter|  100|\n",
      "|  2|   Manel|  150|  2|   Manel|  150|\n",
      "|  2|   Manel|  150|  3|Asdrubal|  200|\n",
      "|  3|Asdrubal|  200|  1|  Valter|  100|\n",
      "|  3|Asdrubal|  200|  2|   Manel|  150|\n",
      "|  3|Asdrubal|  200|  3|Asdrubal|  200|\n",
      "+---+--------+-----+---+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.crossJoin(df).show()# cartesian product \n",
    "# dataframe inside of the function will be the right side of the cartesian product.\n",
    "# Catersian product side is only important because of the order but in practice if order is not a problem side does not metter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+-----+\n",
      "| id|    Name|value|\n",
      "+---+--------+-----+\n",
      "|  1|  Valter|  100|\n",
      "|  2|   Manel|  150|\n",
      "|  3|Asdrubal|  200|\n",
      "|  1|  Valter|  100|\n",
      "|  2|   Manel|  150|\n",
      "|  3|Asdrubal|  200|\n",
      "+---+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_extended = df.union(df).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`df.groupBy()`: Groups the DataFrame using the specified columns, allowing for aggregated operation. I guess that groupBy in dataframes is done as the reduceBy in RDDs once it is the optimized way of doing it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "| id|count|\n",
      "+---+-----+\n",
      "|  1|    1|\n",
      "|  3|    1|\n",
      "|  2|    1|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(df.id).count().show() #Groups the DataFrame using the specified columns, allowing for aggregated operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "it is also possible to use a generalized groupBy function the `.agg()` function which allows to define a dictionary which will map how olumns will be groupBy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "aggregate functions are avg, max, min, sum, count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "all exprs should be Column",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-75-b46f21a7d701>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0magg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'value'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'sum'\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'value'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;34m'max'\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\anaconda3\\lib\\site-packages\\pyspark\\sql\\dataframe.py\u001b[0m in \u001b[0;36magg\u001b[1;34m(self, *exprs)\u001b[0m\n\u001b[0;32m   1444\u001b[0m         \u001b[1;33m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1445\u001b[0m         \"\"\"\n\u001b[1;32m-> 1446\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroupBy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0magg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mexprs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1447\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1448\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0msince\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2.0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\anaconda3\\lib\\site-packages\\pyspark\\sql\\group.py\u001b[0m in \u001b[0;36magg\u001b[1;34m(self, *exprs)\u001b[0m\n\u001b[0;32m    111\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    112\u001b[0m             \u001b[1;31m# Columns\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 113\u001b[1;33m             \u001b[1;32massert\u001b[0m \u001b[0mall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mColumn\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mexprs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"all exprs should be Column\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    114\u001b[0m             jdf = self._jgd.agg(exprs[0]._jc,\n\u001b[0;32m    115\u001b[0m                                 _to_seq(self.sql_ctx._sc, [c._jc for c in exprs[1:]]))\n",
      "\u001b[1;31mAssertionError\u001b[0m: all exprs should be Column"
     ]
    }
   ],
   "source": [
    "df.agg({'value': 'sum'},{'value':'max'}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----------+\n",
      "|sum(value)|max(value)|min(value)|\n",
      "+----------+----------+----------+\n",
      "|       450|       200|       100|\n",
      "+----------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as f\n",
    "df.agg(f.sum(df.value),f.max(df.value),f.min(df.value)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+-----+\n",
      "| id|Name|value|\n",
      "+---+----+-----+\n",
      "|  3|   3|    3|\n",
      "+---+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.agg(*(f.countDistinct(f.col(c)).alias(c) for c in df.columns)).show()\n",
    "#df.agg(f.countDistinct(c)in df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column<b'count(DISTINCT id) AS `id`'> Column<b'count(DISTINCT Name) AS `Name`'> Column<b'count(DISTINCT value) AS `value`'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def printt(x1,x2,x3):\n",
    "    print(x1,x2,x3)\n",
    "    return 1\n",
    "printt(*(f.countDistinct(f.col(c)).alias(c) for c in df.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To save DataFrames, the DataFrameWriter object is used. It behaves similarly to the DataFrameReader, and is able to save into tables or files. \n",
    "\n",
    "The DataFrameWrite(`df.write`) methods include:\n",
    "  - `.format`: specify the source datatype \n",
    "    \n",
    "  - `.mode`: determines the behavior if a directory or tables already exists. Has the following options – error, overwrite, append, ignore (default is error)\n",
    "  \n",
    "  - `.partitionBy`: stores data in partitioned directories in the form of column=value (as with Hive)\n",
    "  \n",
    "  - `.option`: specifies properties for target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.format('parquet').save('test.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: int, Name: string, value: int]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read.parquet('test.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cool functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: int, Name: string, value: int]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.repartition(\"id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dataframe cool functions\n",
    "df = df.repartition(\"country\") # change partitions to be done by country uniques??????\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
